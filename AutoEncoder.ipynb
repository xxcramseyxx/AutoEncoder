{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOypI6UvQkzeyG1wQiK/m3K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xxcramseyxx/autoencoder/blob/main/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndj232AkjVsj",
        "outputId": "46bbfde7-9674-47e3-93e7-fde324930251"
      },
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM80UaZ2JOOj"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.autograd as Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyBrcD5KKBIx"
      },
      "source": [
        "movies = pd.read_csv('/content/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv('/content/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('/content/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYI7YM9KL2Hj"
      },
      "source": [
        "training_set = pd.read_csv('/content/u1.base', delimiter='\\t')\n",
        "training_set = np.array(training_set, dtype=int)\n",
        "test_set = pd.read_csv('/content/u1.test', delimiter='\\t')\n",
        "test_set = np.array(test_set, dtype=int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXzp2TikRro0"
      },
      "source": [
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf0Qf0ZpT1pR"
      },
      "source": [
        "def convert (data):\n",
        "  new_data=[]\n",
        "  for id_users in range(1, nb_users + 1):\n",
        "    id_movies = data[:,1][data[:,0] == id_users]\n",
        "    id_ratings = data[:,2][data[:,0] == id_users]\n",
        "    ratings = np.zeros(nb_movies)\n",
        "    ratings[id_movies - 1] = id_ratings\n",
        "    new_data.append(list(ratings))\n",
        "  return new_data\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at5STFcIOVKS"
      },
      "source": [
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CqAD32APeY1"
      },
      "source": [
        "class SAE(nn.Module):\n",
        "  def __init__(self, ):\n",
        "    super(SAE, self).__init__()\n",
        "    self.fc1 = nn.Linear(nb_movies, 20)\n",
        "    self.fc2 = nn.Linear(20,10) #first number is the number of the last layer of nurons, second is current layer of nurons\n",
        "    self.fc3 = nn.Linear(10,20)\n",
        "    self.fc4 = nn.Linear(20, nb_movies)\n",
        "    self.activation = nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    x = self.activation(self.fc1(x))\n",
        "    x = self.activation(self.fc2(x))\n",
        "    x = self.activation(self.fc3(x))\n",
        "    x = self.fc4(x)\n",
        "    return(x)\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiAUfR_SqSEl",
        "outputId": "e34820cd-f10a-435e-b4d6-b7bdac588801"
      },
      "source": [
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "  train_loss = 0\n",
        "  s = 0.\n",
        "  for id_user in range(nb_users):\n",
        "    input = training_set[id_user].unsqueeze(0) #the batch raised to higher dimension unsqueeze 0=value\n",
        "    target = input.clone() #target is the input vector\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "      output = sae(input)\n",
        "      target.require_grad = False #make sure we don't calculate the GDescent on target\n",
        "      output[target == 0] = 0\n",
        "      loss = criterion(output, target)\n",
        "      mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "      loss.backward()   #direction loss is calculated only used in the training set\n",
        "      train_loss += np.sqrt(loss.data*mean_corrector)\n",
        "      s += 1.\n",
        "      optimizer.step()\n",
        "  print('epoch: {} \\tloss: {}'.format(epoch, train_loss/s))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 \tloss: 0.9133884310722351\n",
            "epoch: 2 \tloss: 0.9133444428443909\n",
            "epoch: 3 \tloss: 0.913264274597168\n",
            "epoch: 4 \tloss: 0.9129523634910583\n",
            "epoch: 5 \tloss: 0.9130857586860657\n",
            "epoch: 6 \tloss: 0.9128329157829285\n",
            "epoch: 7 \tloss: 0.9129331111907959\n",
            "epoch: 8 \tloss: 0.9127558469772339\n",
            "epoch: 9 \tloss: 0.9124163389205933\n",
            "epoch: 10 \tloss: 0.9118534922599792\n",
            "epoch: 11 \tloss: 0.9122657775878906\n",
            "epoch: 12 \tloss: 0.9120137095451355\n",
            "epoch: 13 \tloss: 0.9125962853431702\n",
            "epoch: 14 \tloss: 0.9116908311843872\n",
            "epoch: 15 \tloss: 0.9120516777038574\n",
            "epoch: 16 \tloss: 0.9117342829704285\n",
            "epoch: 17 \tloss: 0.9121155738830566\n",
            "epoch: 18 \tloss: 0.9113150238990784\n",
            "epoch: 19 \tloss: 0.9116933345794678\n",
            "epoch: 20 \tloss: 0.9111431837081909\n",
            "epoch: 21 \tloss: 0.9113439321517944\n",
            "epoch: 22 \tloss: 0.910887598991394\n",
            "epoch: 23 \tloss: 0.9109647870063782\n",
            "epoch: 24 \tloss: 0.9106594324111938\n",
            "epoch: 25 \tloss: 0.9107573628425598\n",
            "epoch: 26 \tloss: 0.9103441834449768\n",
            "epoch: 27 \tloss: 0.9109905958175659\n",
            "epoch: 28 \tloss: 0.9105369448661804\n",
            "epoch: 29 \tloss: 0.9106930494308472\n",
            "epoch: 30 \tloss: 0.9102344512939453\n",
            "epoch: 31 \tloss: 0.9106001257896423\n",
            "epoch: 32 \tloss: 0.90989750623703\n",
            "epoch: 33 \tloss: 0.9099500179290771\n",
            "epoch: 34 \tloss: 0.9096501469612122\n",
            "epoch: 35 \tloss: 0.9096256494522095\n",
            "epoch: 36 \tloss: 0.9094434380531311\n",
            "epoch: 37 \tloss: 0.9093726277351379\n",
            "epoch: 38 \tloss: 0.9088793396949768\n",
            "epoch: 39 \tloss: 0.9088066816329956\n",
            "epoch: 40 \tloss: 0.9087298512458801\n",
            "epoch: 41 \tloss: 0.9091584086418152\n",
            "epoch: 42 \tloss: 0.9086308479309082\n",
            "epoch: 43 \tloss: 0.908283531665802\n",
            "epoch: 44 \tloss: 0.908041775226593\n",
            "epoch: 45 \tloss: 0.9081453084945679\n",
            "epoch: 46 \tloss: 0.9079973101615906\n",
            "epoch: 47 \tloss: 0.9078788161277771\n",
            "epoch: 48 \tloss: 0.9075266122817993\n",
            "epoch: 49 \tloss: 0.9073771238327026\n",
            "epoch: 50 \tloss: 0.9071784019470215\n",
            "epoch: 51 \tloss: 0.9068837761878967\n",
            "epoch: 52 \tloss: 0.9062491059303284\n",
            "epoch: 53 \tloss: 0.9062168002128601\n",
            "epoch: 54 \tloss: 0.9060941338539124\n",
            "epoch: 55 \tloss: 0.9061604142189026\n",
            "epoch: 56 \tloss: 0.9059051275253296\n",
            "epoch: 57 \tloss: 0.9054349064826965\n",
            "epoch: 58 \tloss: 0.9054355025291443\n",
            "epoch: 59 \tloss: 0.9051771759986877\n",
            "epoch: 60 \tloss: 0.9048791527748108\n",
            "epoch: 61 \tloss: 0.9041270017623901\n",
            "epoch: 62 \tloss: 0.9041470289230347\n",
            "epoch: 63 \tloss: 0.9037302136421204\n",
            "epoch: 64 \tloss: 0.9034976363182068\n",
            "epoch: 65 \tloss: 0.9029630422592163\n",
            "epoch: 66 \tloss: 0.9028217792510986\n",
            "epoch: 67 \tloss: 0.9023194313049316\n",
            "epoch: 68 \tloss: 0.9026345610618591\n",
            "epoch: 69 \tloss: 0.9018555879592896\n",
            "epoch: 70 \tloss: 0.9022384285926819\n",
            "epoch: 71 \tloss: 0.9010353088378906\n",
            "epoch: 72 \tloss: 0.9012163877487183\n",
            "epoch: 73 \tloss: 0.9001806378364563\n",
            "epoch: 74 \tloss: 0.9007212519645691\n",
            "epoch: 75 \tloss: 0.8998321294784546\n",
            "epoch: 76 \tloss: 0.8998292088508606\n",
            "epoch: 77 \tloss: 0.8993293642997742\n",
            "epoch: 78 \tloss: 0.8994796276092529\n",
            "epoch: 79 \tloss: 0.8985647559165955\n",
            "epoch: 80 \tloss: 0.8986688256263733\n",
            "epoch: 81 \tloss: 0.8977644443511963\n",
            "epoch: 82 \tloss: 0.8978354334831238\n",
            "epoch: 83 \tloss: 0.8970789313316345\n",
            "epoch: 84 \tloss: 0.896881639957428\n",
            "epoch: 85 \tloss: 0.8966037034988403\n",
            "epoch: 86 \tloss: 0.8966336250305176\n",
            "epoch: 87 \tloss: 0.8956958651542664\n",
            "epoch: 88 \tloss: 0.8952946066856384\n",
            "epoch: 89 \tloss: 0.8947996497154236\n",
            "epoch: 90 \tloss: 0.8947999477386475\n",
            "epoch: 91 \tloss: 0.893774151802063\n",
            "epoch: 92 \tloss: 0.8938050270080566\n",
            "epoch: 93 \tloss: 0.8926962018013\n",
            "epoch: 94 \tloss: 0.8923538327217102\n",
            "epoch: 95 \tloss: 0.8916040062904358\n",
            "epoch: 96 \tloss: 0.8913485407829285\n",
            "epoch: 97 \tloss: 0.8902266621589661\n",
            "epoch: 98 \tloss: 0.8901644349098206\n",
            "epoch: 99 \tloss: 0.889609158039093\n",
            "epoch: 100 \tloss: 0.8894352316856384\n",
            "epoch: 101 \tloss: 0.8885096311569214\n",
            "epoch: 102 \tloss: 0.8885353803634644\n",
            "epoch: 103 \tloss: 0.887374758720398\n",
            "epoch: 104 \tloss: 0.8874242305755615\n",
            "epoch: 105 \tloss: 0.8869991302490234\n",
            "epoch: 106 \tloss: 0.8872275352478027\n",
            "epoch: 107 \tloss: 0.8861766457557678\n",
            "epoch: 108 \tloss: 0.8857895731925964\n",
            "epoch: 109 \tloss: 0.8853917121887207\n",
            "epoch: 110 \tloss: 0.8853482007980347\n",
            "epoch: 111 \tloss: 0.8845097422599792\n",
            "epoch: 112 \tloss: 0.8842376470565796\n",
            "epoch: 113 \tloss: 0.8835408091545105\n",
            "epoch: 114 \tloss: 0.8834003806114197\n",
            "epoch: 115 \tloss: 0.8829894661903381\n",
            "epoch: 116 \tloss: 0.8828398585319519\n",
            "epoch: 117 \tloss: 0.882456362247467\n",
            "epoch: 118 \tloss: 0.8823980689048767\n",
            "epoch: 119 \tloss: 0.8815286159515381\n",
            "epoch: 120 \tloss: 0.8816244006156921\n",
            "epoch: 121 \tloss: 0.8813754916191101\n",
            "epoch: 122 \tloss: 0.881045937538147\n",
            "epoch: 123 \tloss: 0.8808284401893616\n",
            "epoch: 124 \tloss: 0.8846263289451599\n",
            "epoch: 125 \tloss: 0.8817334175109863\n",
            "epoch: 126 \tloss: 0.8803960680961609\n",
            "epoch: 127 \tloss: 0.8792238235473633\n",
            "epoch: 128 \tloss: 0.8789686560630798\n",
            "epoch: 129 \tloss: 0.8784117102622986\n",
            "epoch: 130 \tloss: 0.8785061240196228\n",
            "epoch: 131 \tloss: 0.8782153129577637\n",
            "epoch: 132 \tloss: 0.8774904012680054\n",
            "epoch: 133 \tloss: 0.8773960471153259\n",
            "epoch: 134 \tloss: 0.877608060836792\n",
            "epoch: 135 \tloss: 0.8768197894096375\n",
            "epoch: 136 \tloss: 0.8768719434738159\n",
            "epoch: 137 \tloss: 0.8764621019363403\n",
            "epoch: 138 \tloss: 0.8762083649635315\n",
            "epoch: 139 \tloss: 0.8755316734313965\n",
            "epoch: 140 \tloss: 0.8756438493728638\n",
            "epoch: 141 \tloss: 0.8752009272575378\n",
            "epoch: 142 \tloss: 0.8752188086509705\n",
            "epoch: 143 \tloss: 0.874197244644165\n",
            "epoch: 144 \tloss: 0.8745524883270264\n",
            "epoch: 145 \tloss: 0.8743231892585754\n",
            "epoch: 146 \tloss: 0.8745666146278381\n",
            "epoch: 147 \tloss: 0.8739792704582214\n",
            "epoch: 148 \tloss: 0.874018669128418\n",
            "epoch: 149 \tloss: 0.8733359575271606\n",
            "epoch: 150 \tloss: 0.8733747005462646\n",
            "epoch: 151 \tloss: 0.8727472424507141\n",
            "epoch: 152 \tloss: 0.8728153705596924\n",
            "epoch: 153 \tloss: 0.8725622892379761\n",
            "epoch: 154 \tloss: 0.8725149631500244\n",
            "epoch: 155 \tloss: 0.8721795678138733\n",
            "epoch: 156 \tloss: 0.8719546794891357\n",
            "epoch: 157 \tloss: 0.8716353178024292\n",
            "epoch: 158 \tloss: 0.871403694152832\n",
            "epoch: 159 \tloss: 0.871239960193634\n",
            "epoch: 160 \tloss: 0.8710569739341736\n",
            "epoch: 161 \tloss: 0.8709003329277039\n",
            "epoch: 162 \tloss: 0.8706063628196716\n",
            "epoch: 163 \tloss: 0.8702123761177063\n",
            "epoch: 164 \tloss: 0.8703914880752563\n",
            "epoch: 165 \tloss: 0.8697049617767334\n",
            "epoch: 166 \tloss: 0.8698641061782837\n",
            "epoch: 167 \tloss: 0.868967592716217\n",
            "epoch: 168 \tloss: 0.8686976432800293\n",
            "epoch: 169 \tloss: 0.8688555359840393\n",
            "epoch: 170 \tloss: 0.8684185743331909\n",
            "epoch: 171 \tloss: 0.868289589881897\n",
            "epoch: 172 \tloss: 0.8680301904678345\n",
            "epoch: 173 \tloss: 0.8680390119552612\n",
            "epoch: 174 \tloss: 0.8674009442329407\n",
            "epoch: 175 \tloss: 0.8673651218414307\n",
            "epoch: 176 \tloss: 0.866989254951477\n",
            "epoch: 177 \tloss: 0.8669225573539734\n",
            "epoch: 178 \tloss: 0.8661059141159058\n",
            "epoch: 179 \tloss: 0.8667730093002319\n",
            "epoch: 180 \tloss: 0.8655802607536316\n",
            "epoch: 181 \tloss: 0.8656938076019287\n",
            "epoch: 182 \tloss: 0.8650172352790833\n",
            "epoch: 183 \tloss: 0.8649170994758606\n",
            "epoch: 184 \tloss: 0.8652904629707336\n",
            "epoch: 185 \tloss: 0.8646634817123413\n",
            "epoch: 186 \tloss: 0.8640658259391785\n",
            "epoch: 187 \tloss: 0.8640106320381165\n",
            "epoch: 188 \tloss: 0.8633255362510681\n",
            "epoch: 189 \tloss: 0.8634701371192932\n",
            "epoch: 190 \tloss: 0.8629482388496399\n",
            "epoch: 191 \tloss: 0.8629045486450195\n",
            "epoch: 192 \tloss: 0.8623960614204407\n",
            "epoch: 193 \tloss: 0.8623250126838684\n",
            "epoch: 194 \tloss: 0.8618302345275879\n",
            "epoch: 195 \tloss: 0.8617379665374756\n",
            "epoch: 196 \tloss: 0.8612620830535889\n",
            "epoch: 197 \tloss: 0.8611646294593811\n",
            "epoch: 198 \tloss: 0.8689223527908325\n",
            "epoch: 199 \tloss: 0.869708240032196\n",
            "epoch: 200 \tloss: 0.8617991209030151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8qVyPb7eMLY",
        "outputId": "48a62d56-c3e1-4873-9da5-3f46fd66637d"
      },
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    input = training_set[id_user].unsqueeze(0) #the batch raised to higher dimension unsqueeze 0=value\n",
        "    target = test_set[id_user] #target is the input vector\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "      output = sae(input)\n",
        "      target.require_grad = False #make sure we don't calculate the GDescent on target\n",
        "      output[(target == 0).unsqueeze(0)] = 0\n",
        "      loss = criterion(output, target)\n",
        "      mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "      test_loss += np.sqrt(loss.data*mean_corrector)\n",
        "      s += 1.\n",
        "print('loss: {}'.format(test_loss/s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1682])) that is different to the input size (torch.Size([1, 1682])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss: 0.9525133967399597\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}